# -*- coding: utf-8 -*-
"""GITHUB 2016 Elections Prediction Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rIWzIhB7qutjl66A2l3hqs-Awd_Wd4Vd

# Predicting Voter Tendencies Using County Demographics: Analysis of the 2016 Election

## Prepare model
"""

from matplotlib import pyplot as plt
import matplotlib.ticker as mtick
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import networkx as nx

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Spring 2025/Data Science/Projects/Project 2: Election Data/primary_results.csv')
county_df  = pd.read_csv('/content/drive/MyDrive/Spring 2025/Data Science/Projects/Project 2: Election Data/county_facts.csv')

df.head()

county_df.rename(columns = {
    "PST045214": "Population_2014_estimate",
    "PST040210": "Population_2010_estimate",
    "PST120214": "Population_2010_pct_change",
    "POP010210": "Population_2010",
    "AGE135214": "Under_5_pct_2014",
    "AGE295214": "Under_18_pct_2014",
    "AGE775214": "Above_65_pct_2014",
    "SEX255214": "Female_pct_2014",
    "RHI125214": "White_pct_2014",
    "RHI225214": "African_American_pct_2014",
    "RHI325214": "Native_American_pct_2014",
    "RHI425214": "Asian_pct_2014",
    "RHI525214": "Pacific_Islander_pct_2014",
    "RHI625214": "Two_or_More_Races_pct_2014",
    "RHI725214": "Hispanic_pct_2014",
    "RHI825214": "White_non_Hispanic_pct_2014",
    "POP715213": "Living_same_house_1_year",
    "POP645213": "Foreign_Born_pct_2013",
    "POP815213": "Language_Other_than_English_pct_2009_2013",
    "EDU635213": "High_School_Grad_pct_2009_2013",
    "EDU685213": "Bachelors_Degree_pct_2009_2013",
    "VET605213": "Veterans_2009_2013",
    "LFE305213": "Mean_Travel_Time_to_Work_2009_2013",
    "HSG010214": "Housing_Units_2014",
    "HSG445213": "Homeownership_Rate_2009_2013",
    "HSG096213": "Housing_Units_Multi_Unit_2009_2013",
    "HSG495213": "Housing_Median_Value_2013",
    "HSD410213": "Households_2009_2013",
    "HSD310213": "Persons_Households_2009_2013",
    "INC910213": "Income_Per_Capita_2009_2013",
    "INC110213": "Median_Household_Income_2009_2013",
    "PVY020213": "Persons_Below_Poverty_pct_2009_2013",
    "BZA010213": "Private_Nonfarm_Establishments_2013",
    "BZA110213": "Private_Nonfarm_Employment_2013",
    "BZA115213": "Private_Nonfarm_Employment_pct_Change_2012-2013",
    "NES010213": "NonEmployer_Establishments_2013",
    "SBO001207": "Total_Number_Firms_2007",
    "SBO315207": "Black_Owned_Firms_pct_2007",
    "SBO115207": "American_Indian_Owned_Firms_pct_2007",
    "SBO215207": "Asian_Owned_Firms_pct_2007",
    "SBO515207": "Pacific_Islander_Owned_Firms_pct_2007",
    "SBO415207": "Hispanic_Owned_Firms_pct_2007",
    "SBO015207": "Women_Owned_Firms_pct_2007",
    "MAN450207": "Manufacturers_Shipments_2007",
    "WTN220207": "Merchant_Wholesaler_Sales_2007",
    "RTN130207": "Retail_Sales_2007",
    "RTN131207": "Retail_Sales_per_Capita_2007",
    "AFN120207": "Accommodation_Food_Services_Sales_2007",
    "BPS030214": "Building_Permits_2014",
    "LND110210": "Land_Area_square_miles_2010",
    "POP060210": "Population_square_mile_2010"
}, inplace = True)

county_df["area_name_clean"] = (
    county_df["area_name"]
    .str.replace(r"\s+County$", "", regex=True)  # Removes " County" at the end
    .str.strip()  # Just in case there are extra spaces
)

df = df.dropna(subset=["fips"])

df["fips_int"] = (df["fips"].astype(float).astype(int))

county_df['White_pct_2014'].head()

county_df.shape

"""## Clean county information"""

df_grouped = df.groupby(
    ['state', 'state_abbreviation', 'county', 'fips_int', 'party'],
    as_index=False
).agg({
    'votes': 'sum',
    'fraction_votes': 'sum'
})
df_pivot = df_grouped.pivot(
    index=['state', 'state_abbreviation', 'county', 'fips_int'],
    columns='party',
    values=['votes', 'fraction_votes']
).reset_index()

# df_pivot is multi-index, so this should solve it
df_pivot.columns = [
    '_'.join(col).strip() if isinstance(col, tuple) else col
    for col in df_pivot.columns.values
]

df_pivot.head()

df_pivot.shape

df_pivot['county_'].unique()

"""## Prepare data for merge on a county level"""

df_pivot.isna().sum() / len(df_pivot) * 100

counties_with_missing = df_pivot.loc[df_pivot.isna().any(axis=1), "state_"].unique()
print(counties_with_missing)

county_df[county_df['area_name_clean'].isin(counties_with_missing)]

merged_df = pd.merge(df_pivot, county_df, left_on='fips_int_', right_on='fips', how='left')

# Quick calculation of the winner at each county
merged_df['winner'] = np.where(
    merged_df['votes_Republican'] > merged_df['votes_Democrat'],
    'Republican',
    'Democrat'
)

missing_percent = ((merged_df.isna().sum() / len(merged_df)) * 100).round(1)
missing_values = missing_percent[missing_percent > 0]  # Filter only columns with missing values
missing_values.head()

counties_with_missing = merged_df.loc[merged_df.isna().any(axis=1), "state_"].unique()
print(counties_with_missing)

missing_rows = merged_df[merged_df.isna().any(axis=1)]
missing_rows.head()

merged_df = merged_df.dropna()

area_cols = ['Population_square_mile_2010', 'Land_Area_square_miles_2010']
pop_cols = ['Population_2010', 'Population_2010_pct_change', 'Population_2010_estimate', 'Population_2014_estimate']
to_drop = ['winner_numeric', 'fraction_votes_Republican', 'fraction_votes_Democrat', 'votes_Republican', 'votes_Democrat', 'fips']

numeric_cols = county_df.select_dtypes(include=['int64', 'float64']).columns
numeric_cols = numeric_cols.drop(to_drop, errors='ignore')

"""#### Logistic Model on a county level

`penalty='l1'` = Lasso regression
"""

X = merged_df[numeric_cols]
y = merged_df['winner']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=42)

logr = LogisticRegression(max_iter=1000, class_weight='balanced', penalty='l1', solver='liblinear')
logr.fit(X_train, y_train)

y_pred = logr.predict(X_test)

# Predict probabilities for the positive class (e.g., "Republican")
y_probs = logr.predict_proba(X_test)[:, 1]

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')
precision = precision_score(y_test, y_pred, pos_label='Republican')
recall = recall_score(y_test, y_pred, pos_label='Republican')
f1 = f1_score(y_test, y_pred, pos_label='Republican')
auroc = roc_auc_score(y_test, y_probs)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("AUROC:", auroc)
print("Confusion Matrix:")
print(conf_matrix)

plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix,
    annot=True,
    fmt='.2f',
    cmap='Blues',
    xticklabels=['Democrat', 'Republican'],
    yticklabels=['Democrat', 'Republican']
)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Normalized)')
plt.show()

coeffs = logr.coef_[0]
sorted_indices = np.argsort(coeffs)[::-1]  # Sort indices in descending order

sorted_features = np.array(numeric_cols)[sorted_indices]
sorted_coeffs = coeffs[sorted_indices]

plt.figure(figsize=(12, 12))
plt.barh(numeric_cols, sorted_coeffs)
plt.ylabel("Coefficient Value")
plt.title("Logistic Regression Feature Weights")
plt.show()

grouped_pop = merged_df.groupby("winner")["Population_2014_estimate"].median()

order = ["Democrat", "Republican"]
grouped_pop = grouped_pop.reindex(order)

plt.figure(figsize=(8, 6))
grouped_pop.plot(kind="bar", color=["navy", "red"], edgecolor="black")

plt.xlabel("Winning Party")
plt.ylabel("Median Population")
plt.title("Median Population by Winning Party")
plt.xticks(rotation=0)
plt.show()

sales_columns = [
    "Accommodation_Food_Services_Sales_2007",
    "Retail_Sales_2007",
    "Retail_Sales_per_Capita_2007",
    "Merchant_Wholesaler_Sales_2007"
]

merged_df["sales_total"] = merged_df[sales_columns].sum(axis=1)

#total_sales data vs winning county party

#Calculate the average total sales for counties grouped by "winner" (Republican or Democrat)
avg_sales_by_winner = merged_df.groupby("winner")["sales_total"].mean()

#Create bar chart: average total sales for Democratic vs. Republican counties
plt.figure(figsize=(8, 6))
sns.barplot(x=avg_sales_by_winner.index, y=avg_sales_by_winner.values, palette={"Democrat": "blue", "Republican": "red"})
plt.xlabel("Winning Party")
plt.ylabel("Average Total Sales by County")
plt.title("Comparison of Average Total Sales in Democrat vs. Republican Counties")
plt.show()

"""## VIF Analysis"""

corr_matrix = merged_df[numeric_cols].corr().abs()
threshold = 0.8
G = nx.Graph()
G.add_nodes_from(numeric_cols)

# Add edges for pairs of columns above the threshold
for i in range(len(numeric_cols)):
    for j in range(i + 1, len(numeric_cols)):
        if corr_matrix.iloc[i, j] > threshold:
            G.add_edge(numeric_cols[i], numeric_cols[j])

connected_components = nx.connected_components(G)

# Convert each connected component (a set) into a list
high_corr_groups = [list(cc) for cc in connected_components if len(cc) > 1]

print("Highly correlated groups of features:")
for group in high_corr_groups:
    print(group)

merged_df_clean = merged_df.copy()

# --- Group 1: County Size Variables ---
# These columns are highly correlated because they all measure county size/economic activity.
size_columns = [
    'Population_2010', 'Private_Nonfarm_Employment_2013', 'Households_2009_2013',
    'Retail_Sales_2007', 'Private_Nonfarm_Establishments_2013', 'Population_2014_estimate',
    'Veterans_2009_2013', 'Total_Number_Firms_2007', 'NonEmployer_Establishments_2013',
    'Merchant_Wholesaler_Sales_2007', 'Population_2010_estimate', 'Housing_Units_2014',
    'Accommodation_Food_Services_Sales_2007', 'Manufacturers_Shipments_2007'
]

# Standardize the size-related columns (fill missing with 0, or consider using mean imputation)
scaler = StandardScaler()
size_data = merged_df_clean[size_columns].fillna(0)
size_data_scaled = scaler.fit_transform(size_data)

# Use PCA to reduce to one composite index
pca = PCA(n_components=1)
merged_df_clean["County_Size_Index"] = pca.fit_transform(size_data_scaled)
merged_df_clean.drop(columns=size_columns, inplace=True)

# --- Group 2: Child Demographics ---
if 'Under_18_pct_2014' in merged_df_clean.columns and 'Under_5_pct_2014' in merged_df_clean.columns:
    merged_df_clean["Youth_pct_2014"] = merged_df_clean["Under_18_pct_2014"] + merged_df_clean["Under_5_pct_2014"]
    merged_df_clean.drop(columns=['Under_5_pct_2014', 'Under_18_pct_2014'], inplace=True)

# --- Group 3: Income Measures ---
income_cols = ['Median_Household_Income_2009_2013', 'Income_Per_Capita_2009_2013']
if all(col in merged_df_clean.columns for col in income_cols):
    merged_df_clean["Income_Index"] = merged_df_clean[income_cols].mean(axis=1)
    merged_df_clean.drop(columns=income_cols, inplace=True)

winner_counts = merged_df_clean["winner"].value_counts()
winner_percent = (winner_counts / winner_counts.sum()) * 100

summary_table = pd.DataFrame({
    "Count": winner_counts,
    "Percentage": winner_percent.round(2)
})

summary_table["Percentage"] = summary_table["Percentage"].astype(str) + "%"

print(summary_table)

"""## Logistic Regression Model after VIF"""

numeric_cols_2 = merged_df_clean.select_dtypes(include=['int64', 'float64']).columns
numeric_cols_2 = numeric_cols_2.drop(to_drop, errors='ignore')
numeric_cols_2 = numeric_cols_2.drop('fips_int_', errors='ignore')

merged_df_clean = merged_df_clean.dropna()

X = merged_df_clean[numeric_cols_2]
y = merged_df_clean['winner']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=42)

logr = LogisticRegression(max_iter=1000, class_weight='balanced', penalty='l1', solver='liblinear')
logr.fit(X_train, y_train)

y_pred = logr.predict(X_test)

# Predict probabilities for the positive class (e.g., "Republican")
y_probs = logr.predict_proba(X_test)[:, 1]

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')
precision = precision_score(y_test, y_pred, pos_label='Republican')
recall = recall_score(y_test, y_pred, pos_label='Republican')
f1 = f1_score(y_test, y_pred, pos_label='Republican')
auroc = roc_auc_score(y_test, y_probs)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("AUROC:", auroc)
print("Confusion Matrix:")
print(conf_matrix)

plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix,
    annot=True,
    fmt='.2f',
    cmap='Blues',
    xticklabels=['Democrat', 'Republican'],
    yticklabels=['Democrat', 'Republican']
)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Normalized)')
plt.show()

coeffs = logr.coef_[0]
sorted_indices = np.argsort(coeffs)[::-1]  # Sort indices in descending order

sorted_features = np.array(numeric_cols_2)[sorted_indices]
sorted_coeffs = coeffs[sorted_indices]

plt.figure(figsize=(12, 12))
plt.barh(numeric_cols_2, sorted_coeffs)
plt.ylabel("Coefficient Value")
plt.title("Logistic Regression Feature Weights")
plt.show()

"""## Data exploration"""

plt.figure(figsize=(8, 6))
grouped_density = merged_df_clean.groupby("winner")["Population_square_mile_2010"].mean()

order = ["Democrat", "Republican"]
grouped_density = grouped_density.reindex(order)

grouped_density.plot(
    kind="barh",
    color=["navy", "red"],
    edgecolor="black"
)

plt.xlim(0, 800)
plt.xticks(np.arange(0, 801, 200))

plt.gca().invert_yaxis()

plt.ylabel("Avg Density")
plt.title("Avg Density by Winning Party")
plt.show()

age_columns = ["Under_5_pct_2014", "Under_18_pct_2014", "Above_65_pct_2014"]
grouped_pop = merged_df.groupby("winner")[age_columns].mean()

order = ["Democrat", "Republican"]
grouped_pop = grouped_pop.reindex(order)

plt.figure(figsize=(10, 6))
bar_width = 0.3
x = np.arange(len(age_columns))

plt.bar(x - bar_width/2, grouped_pop.loc["Democrat"], width=bar_width, color="navy", label="Democrat", edgecolor="black")
plt.bar(x + bar_width/2, grouped_pop.loc["Republican"], width=bar_width, color="red", label="Republican", edgecolor="black")

plt.xticks(ticks=x, labels=age_columns, rotation=0)
plt.gca().yaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f%%'))
plt.xlabel("Age Group")
plt.ylabel("Mean Population Percentage")
plt.title("Mean Population Percentage by Age Group and Winning Party")
plt.legend()
plt.show()

total_dem_votes = merged_df_clean["votes_Democrat"].sum()
total_rep_votes = merged_df_clean["votes_Republican"].sum()
total_votes = total_dem_votes + total_rep_votes

dem_pct = (total_dem_votes / total_votes) * 100
rep_pct = (total_rep_votes / total_votes) * 100

vote_summary = pd.DataFrame({
    "Party": ["Democrat", "Republican"],
    "Total Votes": [total_dem_votes, total_rep_votes],
    "Percentage": [f"{dem_pct:.2f}%", f"{rep_pct:.2f}%"]
})

print(vote_summary)